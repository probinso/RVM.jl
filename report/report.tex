\documentclass[11pt]{article} % letterpaper is american!
\usepackage[affil-it]{authblk}

\usepackage[british,UKenglish,USenglish,english,american]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}

\usepackage{amsfonts,amsmath,amsthm,amssymb}

\usepackage{tikz,pgf}
\usetikzlibrary{fit}

\usepackage{csvsimple}

\pagestyle{empty}
\setlength{\parindent}{0mm}
\usepackage[letterpaper, margin=1in]{geometry}
%\usepackage{showframe}

\usepackage{multicol}
\usepackage{enumerate}

\usepackage{verbatim}
\usepackage{listings}

\usepackage{color}

%%
%% Julia definition (c) 2014 Jubobs
%%
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
      end,export,false,for,function,immutable,import,importall,if,in,%
      macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
      using,while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\lstset{%
    language         = Julia,
    basicstyle       = \footnotesize\ttfamily,,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{red},
    showstringspaces = false,
    backgroundcolor  = \color{lightgray},
    numbers          = left,
    title            = \lstname,
    numberstyle      = \tiny\color{lightgray}\ttfamily,
}

\usepackage{xspace}
\usepackage{url}
\usepackage{cite}

\usepackage{coffee4}

\usepackage{titlesec}
\titlespacing*{\subsubsection}{0pt}{*0}{*0}
\titlespacing*{\subsection}{0pt}{0pt}{*0}
\titlespacing*{\section}{0pt}{0pt}{*0}

\newcommand{\Bold}{\mathbf}

\setlength{\parskip}{1em}
\setlength{\parindent}{1em}


\title{Relevance Vector Machine}
\date{\today}
\author{Philip Robinson}
\affil{Oregon Health Sciences University}

\def\irls{iterative reweighted least squares}
\def\IRLS{Iterative reweighted least squares}
\def\rv{relevance vector\xspace}
\def\RVM{Relevance vector machine\xspace}
\def\rvm{\rv machine\xspace}
\def\SVM{Support vector machine\xspace}
\def\svm{support vector machine\xspace}
\def\julia{\texttt{julia}\xspace}
\def\juliaV{\julia \texttt{0.4.7}\xspace}
\def\python{\texttt{Python}\xspace}
\def\MT{Michael Tipping\xspace}
\def\wilt{UCI wilt dataset\xspace}
\def\iris{UCI iris dataset\xspace}


\begin{document}
\maketitle
%\cofeAm{1}{1.0}{0}{5.5cm}{3cm}
%\cofeCm{0.5}{0.5}{180}{0}{0}
%\begin{abstract}
%\end{abstract}

%\section*{General Terms}
%\section*{Keywords}

\section{Introduction}
Due to my interest in \svm{}s, I selected to study \MT's \rvm.
\RVM{}s are sparse models that yield probabilistic predictions
for supervised classification and regression problems. \SVM{}s,
identify a maximal margin hyperplane that separates your data
(with some associated cost parameter). Alternatively, \rvm{}s
fit a Gaussian about this decision boundary, whose variance
are described by fitting \rv{}s. I provide the first \rvm
implementation in \julia.

\subsection{\RVM}
Usually our predictions, modeled by
\[y(\vec{x}; \vec{w}) = \sum_{i=1}^m w_i \Phi(\vec{x})\]
where $m$ counts the general non-linear and fixed basis
functions, described below.
\[\Phi(\vec{x}) = \left[\phi_1(\vec{x}), \dots, \phi_m(\vec{x})\right]^\intercal\]
Our usual goal is to predict $\vec{w}$, our weights.
\MT's paper provides a Bayesian approach for estimating
$\vec{w}$ from this linear form.

Since the \rvm is based on the \svm, we apply this method to
\[y(\vec{x},\vec{w})=\sum_{i=1}^n w_i\cdot K(\vec{x}, \vec{x_i}) + w_0\]
Which shares our same goal (of minimizing critical weights).
We now fit $\vec{\alpha}$, which describe the inverse variance
associated with each weight.

\[p(\vec{w}|\vec{\mu},\vec{\alpha}) = \prod\mathcal{N}(w_i |\mu,\alpha_i^{-1})\]

In both \svm{}s and \rvm{}s, it is completely possible for
every point \(\{\vec{x}_n\}\) to be represented with a with a
weight $w_n$, which should be interpreted as over-fitting. In
\svm{}s, the use of margin/cost parameters dissuade this result.
In \rvm{}s, we achieve the same effect by fitting to a Gaussian
about these weights.

For a potential \rv $\phi(\vec{x_i})$ As $\alpha_i$ becomes large,
$\alpha_i^{-1}$ becomes small, resulting in a non-informative
associated Gaussian. These \rv{}s are down-selected, along
with their associated hyper-parameters.

As this is a classification problem, Once the \rv{}s are identified
and their associated weight Gaussian are fit, a logistic sigmoid
function $\dot{\sigma}$ is applied to the predicted probability.

\[\dot{\sigma}\left(y(\vec{x}; \vec{w})\right)\]

\section{Dataset}
Since I was most interested in classification problems, I
selected to analyze \wilt. This dataset provides summary
statistics of color channels from images of sick and healthy
trees. This remote sensing problem is important to solve, as
it would significantly reduce the costs of assessing the
health of observed trees.

\begin{table}[!h]
\centering
\caption{wilt\_training.csv}
\label{wilt_training.csv}
\begin{tabular}{llllll}
  {\bf class} & {\bf GLCM\_pan}   & {\bf Mean\_Green} & {\bf Mean\_Red}   & {\bf Mean\_NIR}   & {\bf SD\_pan} \\\hline\\
n     & 132.2993039 & 203.4074074 & 90          & 495.4074074 & 36.94377146 \\
n     & 124.0184758 & 170.9272727 & 69.07272727 & 217.8363636 & 13.33725287 \\
w     & 122.7978339 & 202.3428571 & 132.4857143 & 322.2857143 & 15.8353776  \\
w     & 113.6561798 & 213.1071429 & 129.7857143 & 480.8571429 & 29.23776563 \\
w     & 147.7711268 & 220.1111111 & 123.7222222 & 479.1666667 & 23.33703674 \\
n     & 112.8245614 & 236.92      & 108.04      & 705.84      & 22.67065063 \\
w     & 134.4980916 & 210.2121212 & 116.9090909 & 594.8484848 & 27.93768482 \\
w     & 132.4047758 & 201.3055556 & 125.4074074 & 373.8055556 & 23.22029498 \\
%w     & 118.1225854 & 217.8941176 & 134.2352941 & 443.1117647 & 18.37603091 \\
%n     & 127.3958333 & 200.3333333 & 99.16666667 & 486.1666667 & 26.02349366 \\
\vdots
\end{tabular}
\end{table}

The \wilt ships with two files, separating test and training
information. The training set has 74 \texttt{w}ilting trees and
4265 \texttt{n}ot. The test set has 187 \texttt{w}ilting trees
and 313 \texttt{n}ot. The dataset is well described, and has no
missing values.

\section{Implementation Details}
My code was written to most closely resemble the implementation
described in \MT's paper, using the same variable names at all
steps (for clarity and knowledge transfer).

To encourage eventual adoption into the \julia base, I use some
standard libraries as resources. Firstly, I use \texttt{MLBase}
for encoding/decoding of target classes in a standardized way.
Secondly, I use \texttt{MLKernels}\footnote{depends on \juliaV}
as my library of kernels used in machine learning algorithms.

The optimization uses \irls, first solving for $\vec{\mu}$ and
$\Sigma$ for $\vec{w}$ with fixed $\vec{\alpha}$ (by fitting
the posterior probability), then using those values to update
$\vec{\alpha}$ and repeating, until $\vec{\alpha}$ converges.

\subsection{Coding standards}
The when solving for $\vec{\mu}$ and $\Sigma_{M,M}$ is performed
using Newtons Method. All operations were broken out into simple
segmented functions to simplify debugging and increase readability
(exampled below).

\begin{lstlisting}[mathescape=true]
function _newton_method($X_0$::AbstractVector, F::Function, # Initial, Function
                        $\nabla$::Function, $\nabla\nabla$::Function)          # Gradient, Hessian
  while true
    # @show F($X_0$) # uncomment to watch converge
    $X_1$ = $X_0$ - inv($\nabla\nabla(X_0)$) * $\nabla(X_0)$

    # convergence
    $\Delta{}X$ = maximum(abs($X_1$ .- $X_0$))
    $X_0$ = $X_1$[1:end]
    if $\Delta{}X$ < 1e-3
      break
    end
  end
  $X_0$
end
\end{lstlisting}

\subsection{Use and workflow}
To use the system, a \texttt{RVMSpec} is defined by parameters
\begin{itemize}
  \setlength\itemsep{0em}
\item kernel, selected from the \texttt{MLKernels} module
\item maximum iterations, to prevent infinite loops
\item normalization technique, like standard score transformation
\item tolerance, a sufficient delta for $\alpha$ convergence
\item threshold, a sufficient cutoff for assumed infinite inverse variance
\end{itemize}

Next \texttt{fit} on their \texttt{RVMSpec} and data to
generate an \texttt{RVMFit}, which can then be used for predictions.
The {\em normalization technique} is applied to the prediction
data, exactly as it was in the fitting process (ie. standard
score uses the training mean and variance for every feature vector).

\newpage
\section{Evaluation}
\def\linear{\texttt{linear}\xspace}
\def\radial{\texttt{radialbasis}\xspace}
\def\gaussian{\texttt{gaussian}\xspace}
I report the confusion matrix and classification rate and number
of relevance vectors on the \wilt. For this evaluation I trained
against \linear and \gaussian kernels. I also varied the
training sample count of 100, 1000, 2300, 4300. Finally, I include
training times for each test.

\subsection{Language Note}
In my original proposal, I was interested in comparing runtimes
of \python to \julia. This was abandonded because \python has
swappable linear algebra libraries that made it very difficult to
make this useful analysis. I also found that thresholds were a
greater indicator of runtime.

\subsection{\linear}
We can clearly see from the table below that the data isn't linearly
separable.

\begin{table}[!h]
\centering
\caption{\linear}
\label{linear}
\begin{tabular}{l|llll}
{\bf size} & {\bf time (s)} & {\bf \rv{}s} & {\bf classification} & {\bf matrix}\\\hline\\
 100 & 1.898 & 6 & .367& \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 187 & 312\\
  2 & 0 & 1\\
\end{tabular}\\\hline\\
 1000 & 2.057 & 1 & .626 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 0 & 0 \\
  2 & 187 & 313\\
\end{tabular}\\\hline\\
 2300 & 21.162 & 1 & .626 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 0 & 0 \\
  2 & 187 & 313\\
\end{tabular}\\\hline\\
 4300 & 231.225 & 1 & .626& \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 0 & 0 \\
  2 & 187 & 313\\
\end{tabular}\\
\end{tabular}
\end{table}

\newpage
\subsection{\gaussian}
We can see that a \gaussian kernel performs much better than \linear.

\begin{table}[!h]
\centering
\caption{\gaussian}
\label{gaussian}
\begin{tabular}{l|llll}
  {\bf size} & {\bf time (s)}   & {\bf \rv{}s} & {\bf classification} & {\bf matrix}\\\hline\\
 100 & .09 & 5 & .822 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 181 & 83\\
  2 & 6 & 230\\
\end{tabular}\\\hline\\
 1000 & 4.253 & 11 & .824 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 123 & 24\\
  2 & 64 & 289\\
\end{tabular}\\\hline\\
 2300 & 36.53 & 12 & .81 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 112 & 20\\
  2 & 75 & 293\\
\end{tabular}\\\hline\\
 4300 & 257.77 & 15 & .784 & \begin{tabular}{l|ll}
  & 1 & 2 \\\hline
  1 & 99 & 20\\
  2 & 88 & 293\\
\end{tabular}\\
\end{tabular}
\end{table}

\subsection{Conclusion}
After my analysis I found that all the wilted training samples
were at the head of the training file. It would be nice to repeat
this with proportional sampling from these two groups.

\section{Limitations}
There are a few simplifications that were made to complete
this project in time. This implementation only supports
non-informative priors on the fit Gaussian. I also heavily
depend on matrix inversion, which aren't expected to scale.
Finally, I only provide solutions to binary classification.
In future work I would like to add support for multiple
classifications and regression.

\end{document}
