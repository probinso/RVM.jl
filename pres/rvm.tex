\documentclass[10pt]{beamer}
%\documentclass[handout,dvips,11pt,grey]{beamer}
\usetheme{Goettingen}

%\usepackage{tikz,pgf}
\usepackage{multicol}
\usepackage{amsmath,amsthm,amssymb}
%\usepackage{epstopdf}
%\usepackage{xspace}
\usepackage{verbatim}
%\usepackage{circuitikz}
\usepackage{listings}
%\usepackage{graphicx}
%\usepackage{comment}
%\usepackage{array}
\usepackage{coffee4}

\title{Relevance Vector Machine}
\subtitle{Bayesian Extensions to Linear Systems}
\author{Philip Robinson}
\date{\today}
\institute{Presented to \\ OHSU - Machine Learning Class}

\begin{document}

\begin{frame}
  \titlepage
  \cofeAm{0.4}{0.5}{0}{3.5cm}{-2cm}
  \cofeCm{0.5}{0.5}{180}{0}{3cm}
\end{frame}


\begin{frame}{Presentation Overview}
  \cofeAm{0.3}{0.5}{0}{3.5cm}{-2cm}
  \cofeCm{0.4}{0.5}{180}{0}{3cm}

  \begin{itemize}
  \item Introduce SVM
  \item Introduce RVM
  \item Bayesianization
  \item Optimization Problem
  \item Optimization Algorithm
  \item Existing Implementations
  \item Benchmarks
  \item Notes
  \end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines}
  \cofeAm{0.2}{0.5}{0}{3.5cm}{-2cm}
  \cofeCm{0.3}{0.5}{180}{0}{3cm}

  \begin{multicols}{2}
    \includegraphics[width=\columnwidth]{./svm_basics3.png}
    \columnbreak

    SVMs hope to identify a decision boudary protected by the
    greatest margin. Ideally information is linearly seperable, however
    in most cases you must fit a cost parameter. This is usually done
    with guesses or cross validation. {\em (expensive)}
  \end{multicols}

  \begin{itemize}
  \item non-informative classification
  \item only global minimum {\em (even over kernels)}
  \item only Mercer Kernels {\em ... idk}
  \end{itemize}

\end{frame}

\begin{frame}{Relivance Vector Machines}
  \cofeAm{0.1}{0.5}{0}{3.5cm}{-2cm}
  \cofeCm{0.2}{0.5}{180}{0}{3cm}

  \begin{quote}
    RVMs extend SVMs by adding bayesian priors and weights to
    `support vectors` making `relevance vectors`.
  \end{quote}

  \begin{itemize}
  \item Gaussian priors to weights in training \;$\rightarrow$ regression
  \item Use of logistic sigmoid over regression $\rightarrow$ classification
  \item Informative classifications
  \item Preserves global minimum {\em (even over kernels)}
  \item Non-Mercer Kernels {\em ... idk}
  \item {\bf No closed form steps...}
  \end{itemize}

  \begin{quote}
    Priors on vector weights nullify the need for cost parameters
  \end{quote}
\end{frame}

\begin{frame}{Bayesianizatoin}
  \cofeAm{0.05}{0.5}{0}{3.5cm}{-2cm}
  \cofeCm{0.1}{0.5}{180}{0}{3cm}

  \begin{quote}
    
  \end{quote}

  \begin{align*}
    \Phi &= \texttt{user kernel of machine} \\
    \phi_i(\vec{x}) &= \left[1, K(x, x_1), \ldots, K(x, x_N)\right]^T \\
    p(\vec{w}\;| \vec{\alpha}) &= \prod_{i=1}^N \mathcal{N}(w_i| 0, \alpha_i^{-1}) \\
    p(\vec{t}\;|\vec{w}, \sigma^2) &= (2\sigma^2)^{-\frac{N}{2}} exp\left\{- \frac{1}{2\sigma^2}\|\vec{t} - \Phi\vec{w}\|^2 \right\}\\
    p(\vec{w}\;|\vec{t}, \vec{\alpha}, \sigma^2) &= \frac{p(\vec{t}\;|\vec{w}, \sigma^2)p(\vec{w}|\;\vec{\alpha})}{p(\vec{t}\;|\vec{\alpha}, \sigma^2)} \\
    y(\vec{x}; \vec{w}) &= \sum_{i=1}^N w_i \phi_i(\vec{x}) \\
  \end{align*}

  \begin{quote}
    
  \end{quote}
\end{frame}

\begin{frame}{Optimization Problem}
  
\end{frame}

\begin{frame}{Optimization Algorithm}
  \begin{itemize}
  \item Iteratively Reweighted Least Squares (IRLS)
  \begin{itemize}
  \item Gradient Discent
  \item Newton's Method
  \item Gra
  \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Existing Implementations}
\end{frame}

\begin{frame}{Benchmarks}
\end{frame}

\begin{frame}{Notes}
  \begin{itemize}
  \item End of Chapter
  \item Check Defaults
  \item Check Source Material
  \end{itemize}
\end{frame}

\begin{frame}{}
\end{frame}

\begin{frame}{}
\end{frame}

\end{document}
